{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SguU_e2iWV9T",
    "outputId": "e25dfe41-c0df-409f-d25f-a324922a6ada"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSBrGq3M6Sjy"
   },
   "source": [
    "### Stripe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlmQb3eBMsZh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    UNet2DConditionModel,\n",
    "    ControlNetModel,\n",
    "    DDPMScheduler,\n",
    ")\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "If_o9CAzdEoX"
   },
   "source": [
    "### New masking and resizing strategis have been implemented on data generation.\n",
    "\n",
    "\n",
    "        1. Resize so min(H, W) >= check_size while preserving aspect ratio\n",
    "        2. Random crop to (image_size, image_size)\n",
    "        3. Generate striped version + stripe_control on the training. Removed pre-stored image logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rxb4TQf5Mkqw"
   },
   "outputs": [],
   "source": [
    "def make_sparse_row_mask(\n",
    "    H,\n",
    "    W,\n",
    "    skip=None,\n",
    "    skip_options=None,\n",
    "    skip_probs=None,\n",
    "    target_missing_ratio=None,\n",
    "    device=\"cpu\",\n",
    "    dtype=torch.float32,\n",
    "):\n",
    "\n",
    "    # 1) Decide skip value\n",
    "    if target_missing_ratio is not None:\n",
    "        # missing_ratio = 1 - keep_ratio\n",
    "        # keep_ratio = 1 / skip\n",
    "        keep_ratio = max(1.0 - float(target_missing_ratio), 1e-6)\n",
    "        skip = max(int(round(1.0 / keep_ratio)), 1)\n",
    "\n",
    "    elif skip_options is not None:\n",
    "        if skip_probs is None:\n",
    "            skip = random.choice(skip_options)\n",
    "        else:\n",
    "            s = float(sum(skip_probs))\n",
    "            if s <= 0:\n",
    "                raise ValueError(\"sum(skip_probs) must be > 0\")\n",
    "            norm_probs = [p / s for p in skip_probs]\n",
    "            skip = random.choices(skip_options, weights=norm_probs, k=1)[0]\n",
    "    else:\n",
    "        # default if nothing specified\n",
    "        skip = skip or 6\n",
    "\n",
    "    skip = int(skip)\n",
    "\n",
    "    # 2) Random start offset\n",
    "    start = random.randint(0, skip - 1)\n",
    "\n",
    "    # 3) Build row mask\n",
    "    row_mask = torch.zeros(H, dtype=dtype, device=device)\n",
    "    row_mask[start::skip] = 1.0  # keep these rows\n",
    "\n",
    "    # 4) Expand to (1, H, W)\n",
    "    row_mask_2d = row_mask.view(1, H, 1).expand(1, H, W)\n",
    "    return row_mask_2d\n",
    "\n",
    "\n",
    "class StripeControlNetOnlineDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_root,\n",
    "        image_size=512,\n",
    "        check_size=512,\n",
    "        # old-style params (fallback):\n",
    "        min_skip=3,\n",
    "        max_skip=7,\n",
    "        # new SparseRowMasker-style params:\n",
    "        fixed_skip=None,\n",
    "        skip_options=None,\n",
    "        skip_probs=None,\n",
    "        target_missing_ratio=None,\n",
    "        extensions=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_root = Path(image_root)\n",
    "        self.image_size = image_size\n",
    "        self.check_size = check_size\n",
    "\n",
    "        # old-style (uniform between min_skip and max_skip) if new style is not used\n",
    "        self.min_skip = min_skip\n",
    "        self.max_skip = max_skip\n",
    "\n",
    "        # new-style config\n",
    "        self.fixed_skip = fixed_skip\n",
    "        self.skip_options = skip_options\n",
    "        self.skip_probs = skip_probs\n",
    "        self.target_missing_ratio = target_missing_ratio\n",
    "\n",
    "        # Collect all image files\n",
    "        self.files = []\n",
    "        for ext in extensions:\n",
    "            self.files.extend(self.image_root.rglob(f\"*{ext}\"))\n",
    "        self.files = sorted(self.files)\n",
    "\n",
    "        assert len(self.files) > 0, f\"No images found in {self.image_root}\"\n",
    "        print(f\"Found {len(self.files)} images in {self.image_root}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def _resize_if_needed(self, img: Image.Image) -> Image.Image:\n",
    "        W, H = img.size\n",
    "        if H < self.check_size or W < self.check_size:\n",
    "            if H < W:\n",
    "                new_h = self.check_size\n",
    "                new_w = int(W * (new_h / H))\n",
    "            else:\n",
    "                new_w = self.check_size\n",
    "                new_h = int(H * (new_w / W))\n",
    "\n",
    "            img = TF.resize(img, [new_h, new_w], antialias=True)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _random_crop(self, img: Image.Image) -> Image.Image:\n",
    "        W, H = img.size\n",
    "\n",
    "        if H == self.image_size and W == self.image_size:\n",
    "            return img\n",
    "\n",
    "        if H < self.image_size or W < self.image_size:\n",
    "            return TF.center_crop(img, (self.image_size, self.image_size))\n",
    "\n",
    "        top = random.randint(0, H - self.image_size)\n",
    "        left = random.randint(0, W - self.image_size)\n",
    "        return TF.crop(img, top, left, self.image_size, self.image_size)\n",
    "\n",
    "    def _make_stripes(self, clean_t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        clean_t: (3, H, W) in [0,1]\n",
    "        \"\"\"\n",
    "        _, H, W = clean_t.shape\n",
    "        device = clean_t.device\n",
    "        dtype = clean_t.dtype\n",
    "\n",
    "        # Decide whether to use new or old logic\n",
    "        use_new_logic = (\n",
    "            self.fixed_skip is not None\n",
    "            or self.skip_options is not None\n",
    "            or self.target_missing_ratio is not None\n",
    "        )\n",
    "\n",
    "        if use_new_logic:\n",
    "            row_mask_2d = make_sparse_row_mask(\n",
    "                H,\n",
    "                W,\n",
    "                skip=self.fixed_skip,\n",
    "                skip_options=self.skip_options,\n",
    "                skip_probs=self.skip_probs,\n",
    "                target_missing_ratio=self.target_missing_ratio,\n",
    "                device=device,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "        else:\n",
    "            step = random.randint(self.min_skip, self.max_skip)\n",
    "            row_mask = torch.zeros(H, dtype=dtype, device=device)\n",
    "            row_mask[::step] = 1.0\n",
    "            row_mask_2d = row_mask.view(1, H, 1).expand(1, H, W)\n",
    "\n",
    "        striped_t = clean_t * row_mask_2d.expand_as(clean_t)  # (3,H,W)\n",
    "        stripe_control = row_mask_2d.clone()                  # (1,H,W)\n",
    "\n",
    "        return striped_t, stripe_control\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            img_path = self.files[idx]\n",
    "            stem = img_path.stem\n",
    "\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img.load()              # force I/O here\n",
    "                img = img.convert(\"RGB\")\n",
    "            except (UnidentifiedImageError, OSError) as e:\n",
    "                # Bad or temporarily unreadable file: log and pick a new random one\n",
    "                print(f\"[WARN] Skipping unreadable image: {img_path} ({e})\")\n",
    "                idx = random.randint(0, len(self.files) - 1)\n",
    "                continue\n",
    "\n",
    "            # 1) resize if needed (short side < check_size)\n",
    "            img = self._resize_if_needed(img)\n",
    "\n",
    "            # 2) random crop to (image_size, image_size)\n",
    "            img = self._random_crop(img)\n",
    "\n",
    "            # 3) to tensor [0,1]\n",
    "            gt_t = TF.to_tensor(img)  # (3,H,W)\n",
    "\n",
    "            # 4) make stripes + stripe_control\n",
    "            striped_t, stripe_control_t = self._make_stripes(gt_t)  # (3,H,W), (1,H,W)\n",
    "\n",
    "            # 5) ControlNet condition: 4 channels\n",
    "            control_cond = torch.cat([striped_t, stripe_control_t], dim=0)  # (4,H,W)\n",
    "\n",
    "            return {\n",
    "                \"H\": gt_t,\n",
    "                \"control\": control_cond,\n",
    "                \"stem\": stem,\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VW6wxzqkc9QT"
   },
   "source": [
    "## **A little sanity check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFnGfuSpMkl9",
    "outputId": "5774a1e7-d0ca-48a7-f0fc-67618c7f2c6c"
   },
   "outputs": [],
   "source": [
    "train_dataset = StripeControlNetOnlineDataset(\n",
    "    image_root=\"/content/drive/MyDrive/images\",\n",
    "    image_size=512,\n",
    "    check_size=512,\n",
    "    # new masking logic:\n",
    "    skip_options=[3, 4, 5, 6],\n",
    "    skip_probs=[0.15, 0.50, 0.25, 0.10],\n",
    ")\n",
    "\n",
    "print(\"Dataset size:\", len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkLE1EkCMkgW",
    "outputId": "8f43eab4-5a2f-4193-a240-7348e1569c8a"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "gt = batch[\"H\"]          # (B,3,512,512)\n",
    "control = batch[\"control\"]  # (B,4,512,512)\n",
    "\n",
    "print(\"GT shape:\", gt.shape)\n",
    "print(\"Control shape:\", control.shape)\n",
    "print(\"GT range:\", gt.min().item(), gt.max().item())\n",
    "print(\"Control range:\", control.min().item(), control.max().item())\n",
    "print(\"Stem example:\", batch[\"stem\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "1tMskzIgMkdz",
    "outputId": "ea050f81-0a36-453b-fbc8-1541df0a3a6e"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "def show_sample(batch, idx=0):\n",
    "    gt = batch[\"H\"][idx].permute(1, 2, 0).cpu().numpy()          # (H,W,3)\n",
    "    striped = batch[\"control\"][idx, :3].permute(1, 2, 0).cpu().numpy()\n",
    "    mask = batch[\"control\"][idx, 3].cpu().numpy()                # (H,W)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "    axs[0].imshow(gt)\n",
    "    axs[0].set_title(\"GT (resized + random crop)\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(striped)\n",
    "    axs[1].set_title(\"Striped RGB\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].imshow(mask, cmap=\"gray\")\n",
    "    axs[2].set_title(\"Stripe Control Mask\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_sample(batch, 0)\n",
    "show_sample(batch, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDRkGs7KZE6_"
   },
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3IAM9AeMkbX"
   },
   "outputs": [],
   "source": [
    "class StripeTrainingConfig:\n",
    "    pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "    output_dir = \"/content/drive/MyDrive/Experiments_Stripes1/controlnet_stripes_v1\"\n",
    "\n",
    "    # set to None if training from scratch instead of resuming\n",
    "    resume_from = \"/content/drive/MyDrive/Experiments_Stripes1/controlnet_stripes_v1/last_state\"\n",
    "\n",
    "    # Dataset\n",
    "    image_root = \"/content/drive/MyDrive/images\"\n",
    "    image_resolution = 512\n",
    "\n",
    "    # Training\n",
    "    learning_rate = 2e-5\n",
    "    num_train_epochs = 100\n",
    "    train_batch_size = 42\n",
    "    gradient_accumulation_steps = 3\n",
    "\n",
    "    # Scheduler\n",
    "    lr_warmup_steps = 500\n",
    "\n",
    "    # Logging / saving\n",
    "    checkpointing_steps = 1000\n",
    "    mixed_precision = \"bf16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DL8cxPexZaDa"
   },
   "outputs": [],
   "source": [
    "def train_stripe_controlnet(config: StripeTrainingConfig):\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\")\n",
    "    )\n",
    "\n",
    "    # Models\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        config.pretrained_model_name_or_path, subfolder=\"tokenizer\"\n",
    "    )\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        config.pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
    "    )\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        config.pretrained_model_name_or_path, subfolder=\"vae\"\n",
    "    )\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        config.pretrained_model_name_or_path, subfolder=\"unet\"\n",
    "    )\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "        config.pretrained_model_name_or_path, subfolder=\"scheduler\"\n",
    "    )\n",
    "\n",
    "    # ControlNet\n",
    "    if getattr(config, \"resume_from\", None) and os.path.isdir(config.resume_from):\n",
    "        controlnet_path = os.path.join(config.resume_from, \"controlnet\")\n",
    "        print(f\"Resuming ControlNet from checkpoint: {controlnet_path}\")\n",
    "\n",
    "        # Start from UNet\n",
    "        controlnet = ControlNetModel.from_unet(unet)\n",
    "\n",
    "        # Adjust conv_in to 4 channels\n",
    "        original_conv = controlnet.controlnet_cond_embedding.conv_in\n",
    "        new_conv_in = torch.nn.Conv2d(\n",
    "            4,  # 3 striped RGB + 1 stripe_control\n",
    "            original_conv.out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        controlnet.controlnet_cond_embedding.conv_in = new_conv_in\n",
    "\n",
    "        # Load weights\n",
    "        from safetensors.torch import load_file\n",
    "\n",
    "        st_path_sft = os.path.join(controlnet_path, \"diffusion_pytorch_model.safetensors\")\n",
    "        st_path_bin = os.path.join(controlnet_path, \"diffusion_pytorch_model.bin\")\n",
    "\n",
    "        if os.path.exists(st_path_sft):\n",
    "            print(\"Loading weights from .safetensors\")\n",
    "            state_dict = load_file(st_path_sft, device=\"cpu\")\n",
    "        elif os.path.exists(st_path_bin):\n",
    "            print(\"Loading weights from .bin\")\n",
    "            state_dict = torch.load(st_path_bin, map_location=\"cpu\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No weights file found in {controlnet_path}\")\n",
    "\n",
    "        missing, unexpected = controlnet.load_state_dict(state_dict, strict=False)\n",
    "        print(\"Loaded ControlNet state_dict.\")\n",
    "        if missing:\n",
    "            print(\"Missing keys:\", missing)\n",
    "        if unexpected:\n",
    "            print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "    else:\n",
    "        print(\"Initializing NEW ControlNet with 4-channel conditioning...\")\n",
    "        controlnet = ControlNetModel.from_unet(unet)\n",
    "        original_conv = controlnet.controlnet_cond_embedding.conv_in\n",
    "        new_conv_in = torch.nn.Conv2d(\n",
    "            4,\n",
    "            original_conv.out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        controlnet.controlnet_cond_embedding.conv_in = new_conv_in\n",
    "        print(\"âœ… ControlNet conv_in now expects 4 channels.\")\n",
    "\n",
    "    # Freeze base SD parts\n",
    "    vae.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    controlnet.train()\n",
    "\n",
    "    # Dataset & Dataloader\n",
    "    print(\"Setting up Stripe dataset (online generation)...\")\n",
    "    train_dataset = StripeControlNetOnlineDataset(\n",
    "        image_root=config.image_root,\n",
    "        image_size=config.image_resolution,\n",
    "        check_size=config.image_resolution,\n",
    "        # ~25% visibility on average:\n",
    "        skip_options=[3, 4, 5, 6],\n",
    "        skip_probs=[0.15, 0.50, 0.25, 0.10],\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Optimizer & Scheduler\n",
    "    optimizer = torch.optim.AdamW(controlnet.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    max_train_steps = config.num_train_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=config.lr_warmup_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    "    )\n",
    "\n",
    "    # Prepare with Accelerator\n",
    "    controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        controlnet, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    # Dtypes\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "    else:\n",
    "        weight_dtype = torch.float32\n",
    "\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    unet.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "    # Precompute text embeddings\n",
    "    print(\"Precomputing text embeddings...\")\n",
    "    with torch.no_grad():\n",
    "        text_input = tokenizer(\n",
    "            \"a sharp, high-quality photograph\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        prompt_embeds = text_encoder(text_input.input_ids.to(accelerator.device))[0]\n",
    "    print(\"âœ… Text embeddings ready.\")\n",
    "\n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    print(\"ðŸš€ Starting training...\")\n",
    "\n",
    "    for epoch in range(config.num_train_epochs):\n",
    "        controlnet.train()\n",
    "        progress_bar = tqdm(\n",
    "            train_dataloader,\n",
    "            desc=f\"Epoch {epoch+1}/{config.num_train_epochs}\",\n",
    "            disable=not accelerator.is_local_main_process,\n",
    "        )\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            gt = batch[\"H\"].to(accelerator.device)          # (B,3,H,W) in [0,1]\n",
    "            control = batch[\"control\"].to(accelerator.device)  # (B,4,H,W) in [0,1]\n",
    "\n",
    "            # Scale images to [-1,1]\n",
    "            gt = gt * 2.0 - 1.0\n",
    "            control_rgb = control[:, :3, :, :] * 2.0 - 1.0\n",
    "            control_extra = control[:, 3:, :, :]  # stripe_control stays [0,1]\n",
    "            control = torch.cat([control_rgb, control_extra], dim=1)  # (B,4,H,W)\n",
    "\n",
    "            # Encode GT to latents\n",
    "            latents = vae.encode(gt.to(dtype=weight_dtype)).latent_dist.sample()\n",
    "            latents = latents * vae.config.scaling_factor  # (B,4,h,w)\n",
    "\n",
    "            # Sample noise & timesteps\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                noise_scheduler.config.num_train_timesteps,\n",
    "                (latents.shape[0],),\n",
    "                device=latents.device,\n",
    "            ).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Text conditioning (same prompt repeated for batch)\n",
    "            encoder_hidden_states = prompt_embeds.repeat(latents.shape[0], 1, 1)\n",
    "\n",
    "            # ControlNet forward\n",
    "            down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                noisy_latents,\n",
    "                timesteps,\n",
    "                encoder_hidden_states,\n",
    "                controlnet_cond=control.to(dtype=weight_dtype),\n",
    "                return_dict=False,\n",
    "            )\n",
    "\n",
    "            # UNet forward with ControlNet residuals\n",
    "            model_pred = unet(\n",
    "                noisy_latents,\n",
    "                timesteps,\n",
    "                encoder_hidden_states,\n",
    "                down_block_additional_residuals=[\n",
    "                    res.to(dtype=weight_dtype) for res in down_block_res_samples\n",
    "                ],\n",
    "                mid_block_additional_residual=mid_block_res_sample.to(dtype=weight_dtype),\n",
    "            ).sample\n",
    "\n",
    "            # Noise prediction loss\n",
    "            loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            accelerator.clip_grad_norm_(controlnet.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "            progress_bar.set_postfix(\n",
    "                loss=loss.detach().item(),\n",
    "                lr=lr_scheduler.get_last_lr()[0],\n",
    "            )\n",
    "            accelerator.log({\"loss\": loss.detach().item()}, step=global_step)\n",
    "\n",
    "            # Checkpoint\n",
    "            if accelerator.is_main_process and global_step % config.checkpointing_steps == 0:\n",
    "                save_path = os.path.join(config.output_dir, \"last_state\")\n",
    "                accelerator.save_state(save_path)\n",
    "                unwrapped = accelerator.unwrap_model(controlnet)\n",
    "                unwrapped.save_pretrained(os.path.join(save_path, \"controlnet\"))\n",
    "                print(f\"ðŸ’¾ Saved checkpoint at step {global_step}\")\n",
    "\n",
    "        # Epoch checkpoint\n",
    "        if accelerator.is_main_process:\n",
    "            save_path = os.path.join(\n",
    "                config.output_dir, f\"checkpoint-epoch-{epoch+1}\"\n",
    "            )\n",
    "            unwrapped = accelerator.unwrap_model(controlnet)\n",
    "            unwrapped.save_pretrained(save_path)\n",
    "            print(f\"âœ… Epoch {epoch+1} checkpoint saved to {save_path}\")\n",
    "\n",
    "    # Final save\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped = accelerator.unwrap_model(controlnet)\n",
    "        unwrapped.save_pretrained(os.path.join(config.output_dir, \"final_model\"))\n",
    "        print(f\"ðŸŽ‰ Training finished. Final model saved to {config.output_dir}\")\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955,
     "referenced_widgets": [
      "bd158776ffc54388b1af5239d882b7bf",
      "1b113d37edb741209904902ef5b38118",
      "5951a0545eb84ad193a1ab21c564a8fa",
      "8d7965e3d1ee4708be2bb31f6c442cdb",
      "8c8421d1af45448080f63240bcd8b237",
      "0e12aaf952314b80b57348a50502b824",
      "115cbeaddc5342908c3308c649ef35e8",
      "fcae5fe5c29a4b4eb269076f70f555d2",
      "42d8cf57455c43a198378fb912d236f9",
      "e6b11b70d8224b429131037653958515",
      "a79ab6ee753e40d7911a6b4e5a5049e6",
      "d25f9c18df0642f0b0cfbfd17e9a0ece",
      "09bfecf3c7a947498a5bcb50476d843e",
      "2b4a0971d8ed4a91b541dfa6ef247997",
      "3bf8daecc915479891bf41f593ce816b",
      "3b4b329a420747088a28067334ba15ed",
      "e600482d326647ae8132a985a141675e",
      "ec0e6ceab3e04d99a7dd0dd1b12b6a9c",
      "512e92e678ba471a8b4f0d2c9eb41a27",
      "f94a4ff95e6945a2a3daa287fed2cd85",
      "0f46bd18627741d083d1c9a152da80ca",
      "45f839e8843f485d97bed97920339e76",
      "45c9ab9ca3f440ee800f230811cfc8d9",
      "7f10d25f2a7341159c19b63b3f5152ce",
      "ab6524fa3ec64fc096182b4ab8b17855",
      "e461288d155a42d5ba835fec54807e26",
      "d7c7f919e10b48fb8ba1dc93e2fd26c3",
      "257c7b6913d34f9494133ac1ccbc7a3e",
      "dee9658483c14fd7a0021196a63b7eb3",
      "d875a0d270a04d299bf2759feb52e1c7",
      "4a3b61a0e0de413dbea99b526dd51dfc",
      "19800226d20f43e18544c61f3a19fc60",
      "d3d34004b6c14eff91107dfd8baf54d8",
      "9136a17d4bb94f15a990586a3e35330a",
      "2b25b8285828442e8baf5ab87f7ad19c",
      "538f93becc354268a23f07243a94b8c7",
      "c0ceff5414f34f2cb9b2f93bd0661aea",
      "506c8634e60a4018979b2c2207bc83bc",
      "1579e7f776d84f9b983634f60babb751",
      "349151dcb20f4cf3b6cd8620ce7882d6",
      "d29fba15dd1d4390bc62252435e31fc1",
      "8bdf0446c376430bbd4a94549dcf52ee",
      "e1f95dc976014810b55982e31cf02984",
      "18040e9a141347edbc205b391881146c",
      "6b7dcde94b604eebbe04944a108111e1",
      "f9d1636cc6114108ba6db2c81090f50d",
      "85d03691da564546b83b7eff8193ab33",
      "71f8f395a9854fb78e8e9b484d2e842d",
      "ea90497b4e11469d962273bfa0888760",
      "ee06244eeaa44d4ea9ed03a4e3330ac2",
      "75b12c4c10db4a4698611b3a0bf99d2d",
      "c96dbed155744a70ae10f1b2b3654e60",
      "908a20b559b644e88b9ca0167c47b45d",
      "ebe2cf3b07964d1bb233d8c5774bc002",
      "c126f73674124774846655ec7a82c4e6",
      "cd0a003bcd0e4c57a420edcac6383f8a",
      "3487255c062244529ff7248053dd255f",
      "6c707a60258b4aeb9d4c97ec8fcd7d82",
      "3ef21642394548e88fefbb1332976c78",
      "0b765d69b7134b0293da57fb3eaff8b4",
      "456a561d35c041db8d513f53cf2659bd",
      "05accdced1d24449bcfcf52e22c7c9a9",
      "2543ca4f221740428eb0244dc40420f6",
      "dc8981675ed2449f8432bc44ae1f7bcf",
      "a69efcc2bf094959be3c620ef5128213",
      "735d62f343b54509808e26b7b0a8cbac",
      "d64b1cb689e64444aff838e87d502c86",
      "6abd8d6e8e5041a2ae15e2522515262c",
      "db066421cf5f4283aa0bef9d289ad212",
      "51fe192e88d742388e575cba2e33ddaf",
      "338e4aeb2739422ea414e6228643f15c",
      "3d144fcf50a940ebb238d3930dfdbfd9",
      "1b4615017e9947d9b1f5f06cb665c15e",
      "ebf1b5c6e95c4b798cea9e0bc4414785",
      "010f14c8d1aa4bf79d0c1d81e74cede7",
      "6698d79f974542a48e1f1b0fc7e9db05",
      "0a662df619c040d28a14c60b274d6f29",
      "c1da31d2b79a47ad80199bc5869c9d29",
      "2ba763b1deb44f9ca1fefca259413ee3",
      "094d2d80cc6845e6a3b688c1c8658c24",
      "73fc27d8ef1d46c3afc767dc957658bb",
      "305c4cb6c1494249a43483cd4bdfe7c6",
      "4cfce7d42c06438f81da4120e7f95d77",
      "7e16bfe3b99f430faf5f763b2cfcec44",
      "6bfd049c59af42eb8640dc1fe7d4a22c",
      "99d4c1be15ce403eae4b6a989280f713",
      "063e7b1eca1f49cb996dc3b5564773eb",
      "b1b11c6520934b7688fa7a9e7864b192",
      "44485c95ac20483595a24d6e11d38140",
      "41c279e843744dde89a79cf19adb1be0",
      "494dc320b59b4b9e87442e47cf0dffbf",
      "3b1ecf93c4a3468f987a419a0dd0ecc7",
      "949e9cdaa3804ee19f47ee6ca4f1b9b8",
      "bdd8b40eef02487eb6ee545b1f08362c",
      "ee033b53dc1e49278491a9a24ee15eb9",
      "42ad01f13de94b8dbcc169afd3d344d9",
      "a6ee79b4262847eb8eb336dee329f50a",
      "4476c5a2dbeb4222bc2532de4416c5b7",
      "aeb80a94369448f191f1471599f5602b",
      "7eaea2cd2d7a4973b718016d2b538250",
      "491b9954425e4882b7d46bb0bb3f4d54",
      "4f4abc6b96314fbbbe2fedf7c3447260",
      "b7137d12681142da8f0d0f82e2d4378c",
      "33427cb2acc2485ba5c336baa4c17d08",
      "fc03037adb684e28ae3b1e5c87463633",
      "e3df516b40f14b5f98f965e76064b0fa",
      "94c7d8cfac28402b82bf9b1b2094aef7",
      "fc93466ec480405394c64005f7521570",
      "32c292107c5946b0ae7d8e87aadee377",
      "089402ae8f3443f3956c76ed4b675ef6",
      "100b6d39c47e429e856ad14ef0b38823",
      "5e486c9bc1d14680a3fd2a837ef6a7dc",
      "c8c8f5ee60c349dc8f36a8064ff7caa3",
      "1ab310474b91428c9211d9f5007f0a95",
      "64ffade124724d7c9c0e9924894288ae",
      "441b725aa3304f6b99768bfe2c755d50",
      "dd91f31392ec4f0f9b324fc70ac584d0",
      "266c279a45904c3e9d886bbd7694071d",
      "b330805f25f04822be097f35a4339f6c",
      "7b4e6cdca7a54e3bbda37e0b669155db",
      "75e9425a704c401dadf2731487b5135b",
      "16f2f328b315403b8c8a84b5abe0156e",
      "540216392c9a4724b0b3e6e54c522081",
      "0b7ab0d9a7a4424ba87d129d6ad13908",
      "f41031fc07784da696e0e66ce1d435d1",
      "e520d42651254de497a5228e051c0bef",
      "caa55ba6c31e4d4e9f7fc00673385268",
      "4ed4d7cfdc6546008e16875fc7a82031",
      "879197d000a749e797989278884fcb22",
      "be3e245ec5b148199dcd1fedeaafbca9",
      "24c6917649f445c79f6b942ff7710754",
      "ddfe1a76df1040cb9b28e4ec19ae4660"
     ]
    },
    "id": "fdeDkzx3boSm",
    "outputId": "ef5ef05d-d748-4ee0-f40c-5acd9d90f48c"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cfg = StripeTrainingConfig()\n",
    "    num_gpus = 1\n",
    "    notebook_launcher(train_stripe_controlnet, args=(cfg,), num_processes=num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-liLlDQrb8KY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
